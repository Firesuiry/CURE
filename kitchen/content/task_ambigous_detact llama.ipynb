{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TEST_FLAG = os.getenv('TEST_FLAG', '0') == '1'\n",
    "cache_dir = os.getenv('CACHE_DIR', '0')\n",
    "if cache_dir == \"0\":\n",
    "    cache_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown A few imports and downloading data\n",
    "# !pip install -U --no-cache-dir gdown --pre\n",
    "# !pip install openai tqdm\n",
    "!export HF_ENDPOINT=https://hf-mirror.com\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "# import openai\n",
    "import signal\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pdb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "class LLAMA:\n",
    "    def __init__(self, model_name,load_in_8bit=False):\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                          low_cpu_mem_usage=True, \n",
    "                                                          torch_dtype=torch.float16, \n",
    "                                                          device_map=\"auto\",\n",
    "                                                          cache_dir=cache_dir,\n",
    "                                                          load_in_8bit=load_in_8bit)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=cache_dir)\n",
    "\n",
    "    def llama(self, prompt, max_length=256, output_scores=False, processors=None, temperature=1.0, stop_seq=None, skip_inputs=True):\n",
    "        inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(self.device)\n",
    "            # 停止序列处理\n",
    "        generation_kwargs = {}\n",
    "        if stop_seq:\n",
    "            stop_token_ids = self.tokenizer(stop_seq, add_special_tokens=False).input_ids\n",
    "\n",
    "            # Define stopping criteria\n",
    "            from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "            class StopOnTokenCriteria(StoppingCriteria):\n",
    "                def __init__(self, stop_sequences):\n",
    "                    # stop_sequence should be a list of token IDs representing \\N\\N\n",
    "                    self.stop_sequences = stop_sequences\n",
    "\n",
    "                def __call__(self, input_ids, scores, **kwargs):\n",
    "                    # Check if the end of input_ids matches the stop_sequence\n",
    "                    for stop_sequence in self.stop_sequences:  \n",
    "                        if len(input_ids[0]) >= len(stop_sequence):  # Ensure there are enough tokens to compare\n",
    "                            if input_ids[0, -len(stop_sequence):].tolist() == stop_sequence:\n",
    "                                return True\n",
    "                    return False\n",
    "\n",
    "            generation_kwargs[\"stopping_criteria\"] = StoppingCriteriaList([StopOnTokenCriteria(stop_token_ids)])\n",
    "        outputs = self.model.generate(**inputs, logits_processor=processors, \n",
    "                                max_length=inputs.input_ids.size(1) + max_length, \n",
    "                                return_dict_in_generate=True, \n",
    "                                output_scores=output_scores, \n",
    "                                temperature=temperature, \n",
    "                                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                                do_sample=False,\n",
    "                                **generation_kwargs)\n",
    "        \n",
    "        if skip_inputs:# 将output中的input删除，只保留新生成的output\n",
    "            new_generate_sequence = outputs.sequences[0, inputs.input_ids.size(1):]\n",
    "            decoded_output = self.tokenizer.decode(new_generate_sequence)\n",
    "        else:\n",
    "            decoded_output = self.tokenizer.decode(outputs.sequences[0])\n",
    "        return outputs, decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.logits_process import InfNanRemoveLogitsProcessor\n",
    "from transformers import LogitsProcessorList, LogitsProcessor\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    llama_obj\n",
    "except:\n",
    "    model_name = \"unsloth/Llama-3.3-70B-Instruct\"\n",
    "    if TEST_FLAG:\n",
    "        model_name = \"voidful/Llama-3.2-8B-Instruct\"\n",
    "    llama_obj = LLAMA(model_name, load_in_8bit=True)\n",
    "    llama = llama_obj.llama\n",
    "\n",
    "    tokenizer = llama_obj.tokenizer\n",
    "    class RestrictTokenLogitsProcessor(LogitsProcessor):\n",
    "        def __init__(self, tokenizer, allowed_tokens):\n",
    "            self.allowed_token_ids = tokenizer.convert_tokens_to_ids(allowed_tokens)\n",
    "\n",
    "        def __call__(self, input_ids, scores):\n",
    "            # Set logits of all tokens except the allowed ones to -inf\n",
    "            forbidden_tokens_mask = torch.ones_like(scores).bool()\n",
    "            forbidden_tokens_mask[:, self.allowed_token_ids] = False\n",
    "            scores[forbidden_tokens_mask] = float('-inf')\n",
    "            return scores\n",
    "\n",
    "    allowed_tokens = ['A', 'B', 'C', 'D', 'E']\n",
    "    allowed_token_ids = tokenizer.convert_tokens_to_ids(allowed_tokens)\n",
    "    processors = LogitsProcessorList([\n",
    "        RestrictTokenLogitsProcessor(tokenizer, allowed_tokens),\n",
    "        InfNanRemoveLogitsProcessor()  # Removes inf/nan values to prevent errors during generation\n",
    "    ])\n",
    "\n",
    "    yes_no_processors = LogitsProcessorList([\n",
    "        RestrictTokenLogitsProcessor(tokenizer, ['Yes', 'No']),\n",
    "        InfNanRemoveLogitsProcessor()  # Removes inf/nan values to prevent errors during generation\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = llama_obj.tokenizer\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "def chat(messages, max_length=1000, temperature=0.0, do_sample=True):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(llama_obj.device)\n",
    "    decode_outputs = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "    outputs = llama_obj.model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_length,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=do_sample,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': response\n",
    "    })\n",
    "    return response, messages\n",
    "\n",
    "def chat_with_score(messages, max_length=1, allowed_token_ids=['Yes', 'No']):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(llama_obj.device)\n",
    "    logits_processor = LogitsProcessorList([\n",
    "    RestrictTokenLogitsProcessor(tokenizer, allowed_token_ids),\n",
    "    InfNanRemoveLogitsProcessor()  # Removes inf/nan values to prevent errors during generation\n",
    "    ])\n",
    "    outputs = llama_obj.model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_length,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        logits_processor=logits_processor,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True, \n",
    "    )\n",
    "    last_token_logits = outputs.scores[-1]\n",
    "    last_token_logits = last_token_logits.detach().cpu()\n",
    "    probs = torch.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "    all_tokens = ['Yes', 'No']\n",
    "    allowed_token_ids = tokenizer.convert_tokens_to_ids(all_tokens)\n",
    "    token_probs = {}\n",
    "    for j in range(len(all_tokens)):\n",
    "        log_prob = probs[0, allowed_token_ids[j]].item()\n",
    "        token_probs[all_tokens[j]] = log_prob\n",
    "    return token_probs\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a assistent, please reply according to the user\\'s request.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '1+1=2?say Yes or No'\n",
    "    }\n",
    "]\n",
    "print(chat(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scenario_info_path = 'metabot-tasks-info.txt'\n",
    "with open(scenario_info_path, 'r') as f:\n",
    "    scenario_info_text = f.read()\n",
    "scenario_info_text = scenario_info_text.split('\\n\\n')\n",
    "print('Loaded scenario info from ' + scenario_info_path)\n",
    "#@title\n",
    "# Print a scenario\n",
    "print('Sample scenario:\\n')\n",
    "print(scenario_info_text[0].split('\\n',1)[1])  # remove the printed index\n",
    "'''\n",
    "Scene: a Coke, a bottled unsweetened tea, and a Sprite\n",
    "Task: Bring me a flavored drink.\n",
    "User intent (object): Coke, bottled unsweetened tea, Sprite\n",
    "User intent (location): pick-up\n",
    "Scene objects: Coke, bottled unsweetened tea, Sprite\n",
    "Task category: creative_multilabel_task'''\n",
    "\n",
    "scenario_data = []\n",
    "for scenario in scenario_info_text:\n",
    "    if len(scenario.split('\\n')) < 7:\n",
    "        continue\n",
    "    index = int(scenario.split('\\n')[0])\n",
    "    scenario = scenario.split('\\n')[1:]\n",
    "    scene = scenario[0].split('Scene: ')[1]\n",
    "    task = scenario[1].split('Task: ')[1]\n",
    "    user_intent_object = scenario[2].split('User intent (object): ')[1]\n",
    "    user_intent_location = scenario[3].split('User intent (location): ')[1]\n",
    "    scene_objects = scenario[4].split('Scene objects: ')[1]\n",
    "    task_category = scenario[5].split('Task category: ')[1]\n",
    "    scenario_data.append({\n",
    "            'index': index,\n",
    "            'scene': scene,\n",
    "            'task': task,\n",
    "            'user_intent_object': user_intent_object,\n",
    "            'user_intent_location': user_intent_location,\n",
    "            'scene_objects': scene_objects,\n",
    "            'task_category': task_category\n",
    "    })\n",
    "\n",
    "print(len(scenario_data))\n",
    "print(scenario_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run with api\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import requests\n",
    "prompt = '''\n",
    "You are a robot in kitchen. You are given a scenario and a task. You need to place an item in a new location.\n",
    "Scenario: {scene}\n",
    "Task: {task}\n",
    "Optional items are:  {scene}\n",
    "Optional target locations are:  {scene}, user`s hand, top drawer, bottom drawer, garbage can.\n",
    "Please tell me what items you choose from and where is the target locations. You can choose one or more. You need to choose every item and target location fit the task.\n",
    "(Based on common-sense reasoning, extreme special cases should be disregarded.) '''\n",
    "\n",
    "API_KEY = '0'\n",
    "url = 'https://cloud.infini-ai.com/maas/v1/chat/completions'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {API_KEY}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# 确保缓存目录存在\n",
    "os.makedirs('cache', exist_ok=True)\n",
    "\n",
    "def fetch(prompt):\n",
    "    data = {\n",
    "        'model': 'llama-3.3-70b-instruct',\n",
    "        'messages': [\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    try:\n",
    "        response_data = response.json()\n",
    "    except json.JSONDecodeError:\n",
    "        print(response.text)\n",
    "        return None\n",
    "    return response_data['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "def ambigous_check(scenario):\n",
    "    scence = scenario['scene']\n",
    "    task = scenario['task']\n",
    "    input_prompt = prompt.format(scene=scence, task=task)\n",
    "    print(input_prompt)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    res = fetch(input_prompt)\n",
    "    print(res)\n",
    "    new_p = f'''\n",
    "    The following is a robot's thought process; how many items he choose? and how many locations he choose? Please answer with only two numbers, separated by a space, with the item number first.\n",
    "    {res}\n",
    "    '''\n",
    "    res2 = fetch(new_p)\n",
    "    print(res2)\n",
    "    nums = res2.split(' ')\n",
    "    if int(nums[0]) == 1 and int(nums[1]) == 1:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run local\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "prompt = '''\n",
    "You are a robot in kitchen. You are given a scenario and a task. You need to place an item in a new location.\n",
    "Scenario: {scene}\n",
    "Task: {task}\n",
    "Optional items are:  {scene}\n",
    "Optional target locations are:  {scene}, user`s hand, top drawer, bottom drawer, garbage can.\n",
    "Please tell me what items you choose from and where is the target locations. You can choose one or more. You need to choose every item and target location fit the task.\n",
    "(Based on common-sense reasoning, extreme special cases should be disregarded.) '''\n",
    "\n",
    "def ambigous_check(scenario):\n",
    "    scence = scenario['scene']\n",
    "    task = scenario['task']\n",
    "    input_prompt = prompt.format(scene=scence, task=task)\n",
    "    print(input_prompt)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    res = chat(messages, do_sample=False)[0]\n",
    "    print(res)\n",
    "    new_p = f'''\n",
    "    The following is a robot's thought process; how many items he choose? and how many locations he choose? Please answer with only two numbers, separated by a space, with the item number first.\n",
    "    {res}\n",
    "    '''\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": new_p\n",
    "        }\n",
    "    ]\n",
    "    res2 = chat(messages, do_sample=False)[0]\n",
    "    print(res2)\n",
    "    nums = res2.split(' ')\n",
    "    amb = False\n",
    "    if int(nums[0]) == 1 and int(nums[1]) == 1:\n",
    "        amb = False\n",
    "    else:\n",
    "        amb = True\n",
    "    result = {\n",
    "        'ambiguous': amb\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "ambigous_check(scenario_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "fail_list = []\n",
    "task_len = 300\n",
    "if TEST_FLAG: task_len = 5\n",
    "for i in tqdm.trange(task_len):\n",
    "    file_path = f'task_data_llama/{i}.json'\n",
    "    if os.path.exists(file_path):\n",
    "        continue\n",
    "    res = ambigous_check(scenario_data[i])\n",
    "    if res is None:\n",
    "        fail_list.append(i)\n",
    "        continue\n",
    "    scenario_data[i].update(res)\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(scenario_data[i], f, indent=1)\n",
    "print(fail_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
